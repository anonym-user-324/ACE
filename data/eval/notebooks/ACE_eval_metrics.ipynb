{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e03708c",
   "metadata": {},
   "source": [
    "# ACE Evaluation (Dataset → Embeddings → Metrics)\n",
    "\n",
    "This notebook loads the ACE events dataset, builds embeddings, rolls episodes, and reproduces temporal metrics.\n",
    "It assumes FAISS is installed (recommended for speed), but will fall back to NumPy if not.\n",
    "\n",
    "\n",
    "Notes:\n",
    "- This notebook produces dataset-only baselines (no live rag events).\n",
    "- If you use pool_candidates.csv, ensure event_id values match ACE dataset IDs (ace-<qid>) before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669069fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import calendar\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "REPO_ROOT = None\n",
    "for candidate in [Path.cwd().resolve(), *Path.cwd().resolve().parents]:\n",
    "    if (candidate / 'src').exists():\n",
    "        REPO_ROOT = candidate\n",
    "        break\n",
    "if REPO_ROOT is None:\n",
    "    raise RuntimeError('Could not find repo root containing src/')\n",
    "sys.path.append(str(REPO_ROOT))\n",
    "\n",
    "import src.storage_helpers as storage\n",
    "\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "EVENTS_PATH = DATA_DIR / \"events\" / \"ace_events_h1_2025.jsonl\"\n",
    "EMBED_DIR = DATA_DIR / \"embeddings\"\n",
    "EMBED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMBED_MODEL_NAME = \"intfloat/e5-large-v2\" # Used for current test/results\n",
    "# EMBED_MODEL_NAME = \"intfloat/e5-small-v2\" # smaller/faster for limited resources - results may vary\n",
    "EMBED_QUERY_PREFIX = \"query: \"\n",
    "EMBED_PASSAGE_PREFIX = \"passage: \"\n",
    "EMBED_MODEL_SLUG = EMBED_MODEL_NAME.replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "EMBED_PATH = EMBED_DIR / f\"event_embeddings_{EMBED_MODEL_SLUG}.npz\"\n",
    "\n",
    "EVAL_DIR = DATA_DIR / \"eval\"\n",
    "INPUT_DIR = EVAL_DIR / \"inputs\"\n",
    "OUTPUT_DIR = EVAL_DIR / \"outputs\"\n",
    "METRICS_DIR = OUTPUT_DIR / \"metrics\"\n",
    "AUDIT_DIR = OUTPUT_DIR / \"audit\"\n",
    "MANIFEST_DIR = OUTPUT_DIR / \"manifests\"\n",
    "\n",
    "for path in [INPUT_DIR, METRICS_DIR, AUDIT_DIR, MANIFEST_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "TEMPORAL_QUERY_PATH = INPUT_DIR / \"temporal_queries.json\"\n",
    "RELATIVE_QUERY_PATH = INPUT_DIR / \"relative_temporal_queries.json\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "REBUILD_EMBEDDINGS = False # Set to True force re-building embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility settings\n",
    "EVAL_SEED = 13\n",
    "print(\"Eval seed:\", EVAL_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a075b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run manifest (reproducibility metadata)\n",
    "import hashlib\n",
    "import platform\n",
    "\n",
    "RUN_MANIFEST_PATH = MANIFEST_DIR / \"run_manifest.json\"\n",
    "manifest = {\n",
    "    \"timestamp_utc\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"dataset_id\": \"anon-user-423/ACE\",\n",
    "    \"dataset_config\": \"events\",\n",
    "    \"embed_model\": EMBED_MODEL_NAME,\n",
    "    \"seed\": EVAL_SEED,\n",
    "    \"python\": platform.python_version(),\n",
    "}\n",
    "system_prompt = globals().get(\"SYSTEM_PROMPT\", \"\")\n",
    "if system_prompt:\n",
    "    manifest[\"system_prompt_sha256\"] = hashlib.sha256(system_prompt.encode()).hexdigest()\n",
    "RUN_MANIFEST_PATH = MANIFEST_DIR / \"run_manifest.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d368010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ds = load_dataset(\"anon-user-423/ACE\", \"events\", split=\"train\")\n",
    "print(ds)\n",
    "print(\"Columns:\", ds.column_names)\n",
    "print(\"Sample:\", ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f253525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write events to JSONL once (used by storage helpers)\n",
    "EVENTS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "if not EVENTS_PATH.exists():\n",
    "    with EVENTS_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for row in ds:\n",
    "            f.write(json.dumps(dict(row), ensure_ascii=True) + \"\\n\")\n",
    "    print(\"Wrote\", EVENTS_PATH)\n",
    "else:\n",
    "    print(\"Events file already exists:\", EVENTS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build episodes from dataset\n",
    "\n",
    "# ------ Will take around an hour or two to run ------\n",
    "\n",
    "episode_dir = DATA_DIR / \"episodes\"\n",
    "if not episode_dir.exists() or not any(episode_dir.rglob(\"*.json\")):\n",
    "    summary = storage.roll_up_episodes(events_path=EVENTS_PATH, destination_dir=episode_dir, overwrite=True, show_progress=True)\n",
    "    print(\"Episode roll-up:\", summary)\n",
    "else:\n",
    "    print(\"Episodes already present:\", episode_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b673e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build or load embeddings\n",
    "if REBUILD_EMBEDDINGS or not EMBED_PATH.exists():\n",
    "    model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "    event_ids = []\n",
    "    vectors = []\n",
    "    for start in tqdm(range(0, len(ds), BATCH_SIZE), desc=\"Embedding events\"):\n",
    "        batch = ds[start:start + BATCH_SIZE]\n",
    "        texts = [f\"{EMBED_PASSAGE_PREFIX}{q} {r}\".strip() for q, r in zip(batch[\"question\"], batch[\"response\"]) ]\n",
    "        emb = model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "        vectors.append(emb)\n",
    "        event_ids.extend(batch[\"event_id\"])\n",
    "    vectors = np.vstack(vectors)\n",
    "    np.savez(EMBED_PATH, event_ids=np.array(event_ids), vectors=vectors)\n",
    "    print(\"Wrote\", EMBED_PATH)\n",
    "else:\n",
    "    model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "    print(\"Using cached embeddings:\", EMBED_PATH)\n",
    "\n",
    "\n",
    "storage.load_event_embeddings_npz(EMBED_PATH)\n",
    "storage.register_event_embedder(lambda text: model.encode([f\"{EMBED_QUERY_PREFIX}{text}\"], convert_to_numpy=True)[0])\n",
    "print('Event embeddings loaded:', len(storage.EVENT_EMBEDDINGS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061faecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense-only sanity check (embeddings + embedder)\n",
    "print(\"Event embeddings loaded:\", len(storage.EVENT_EMBEDDINGS))\n",
    "print(\"Event embedder set:\", storage.EVENT_EMBEDDER is not None)\n",
    "if not storage.EVENT_EMBEDDINGS:\n",
    "    print(\"No event embeddings loaded: dense-only will be zero.\")\n",
    "elif storage.EVENT_EMBEDDER is None:\n",
    "    print(\"No event embedder registered: dense-only will be zero.\")\n",
    "else:\n",
    "    try:\n",
    "        vec = storage.EVENT_EMBEDDER(\"sanity check\")\n",
    "        print(\"Sample embedder output shape:\", getattr(vec, \"shape\", None))\n",
    "    except Exception as exc:\n",
    "        print(\"Embedder call failed:\", exc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build retrieval index\n",
    "index = storage.RetrievalIndex.build(\n",
    "    primary_path=DATA_DIR / \"events\" / \"custom_events.jsonl\",\n",
    "    extra_paths=[EVENTS_PATH],\n",
    "    show_progress=True,\n",
    ")\n",
    "print(\"Index events:\", len(index.events))\n",
    "print(\"FAISS enabled:\", index.faiss_index is not None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32acea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build retriever variants\n",
    "BUILD_VARIANT_INDEXES = True\n",
    "LEXICAL_WEIGHT = 0.1\n",
    "TIME_WEIGHT = 0.2\n",
    "HALF_LIFE_DAYS = 30\n",
    "\n",
    "def _build_index(lexical_weight, time_weight, use_dense=True):\n",
    "    saved = None\n",
    "    if not use_dense:\n",
    "        saved = dict(storage.EVENT_EMBEDDINGS)\n",
    "        storage.EVENT_EMBEDDINGS.clear()\n",
    "    idx = storage.RetrievalIndex.build(\n",
    "        primary_path=DATA_DIR / \"events\" / \"custom_events.jsonl\",\n",
    "        extra_paths=[EVENTS_PATH],\n",
    "        half_life_days=HALF_LIFE_DAYS,\n",
    "        lexical_weight=lexical_weight,\n",
    "        time_weight=time_weight,\n",
    "    )\n",
    "    if saved is not None:\n",
    "        storage.EVENT_EMBEDDINGS.update(saved)\n",
    "    return idx\n",
    "\n",
    "if BUILD_VARIANT_INDEXES:\n",
    "    idx_hybrid = _build_index(LEXICAL_WEIGHT, TIME_WEIGHT, use_dense=True)\n",
    "    idx_hybrid_no_time = _build_index(LEXICAL_WEIGHT, 0.0, use_dense=True)\n",
    "    idx_dense_only = _build_index(0.0, 0.0, use_dense=True)\n",
    "    idx_lexical_only = _build_index(LEXICAL_WEIGHT, 0.0, use_dense=False)\n",
    "else:\n",
    "    idx_hybrid = index\n",
    "    idx_hybrid_no_time = index\n",
    "    idx_dense_only = index\n",
    "    idx_lexical_only = index\n",
    "\n",
    "RETRIEVERS = {\n",
    "    \"hybrid\": lambda q, k: idx_hybrid.search(q, limit=k),\n",
    "    \"hybrid_no_time\": lambda q, k: idx_hybrid_no_time.search(q, limit=k),\n",
    "    \"dense_only\": lambda q, k: idx_dense_only.search(q, limit=k),\n",
    "    \"bm25\": lambda q, k: idx_lexical_only.search(q, limit=k),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense coverage check (index + embeddings alignment)\n",
    "print(\"Index events:\", len(idx_dense_only.events))\n",
    "print(\"Embeddings loaded:\", len(storage.EVENT_EMBEDDINGS))\n",
    "print(\"Dense ids:\", len(idx_dense_only.dense_ids))\n",
    "print(\"Dense vectors shape:\", getattr(idx_dense_only.dense_vectors, \"shape\", None))\n",
    "print(\"FAISS present:\", idx_dense_only.faiss_index is not None)\n",
    "\n",
    "missing = [e.get(\"event_id\") for e in idx_dense_only.events if e.get(\"event_id\") not in storage.EVENT_EMBEDDINGS]\n",
    "print(\"Missing embeddings:\", len(missing))\n",
    "if missing:\n",
    "    print(\"Missing sample:\", missing[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load eval queries\n",
    "absolute_queries = json.loads(TEMPORAL_QUERY_PATH.read_text())\n",
    "relative_queries = json.loads(RELATIVE_QUERY_PATH.read_text())\n",
    "print(\"Absolute queries:\", len(absolute_queries))\n",
    "print(\"Relative queries:\", len(relative_queries))\n",
    "\n",
    "if relative_queries and 'relative' in relative_queries[0]:\n",
    "    ref_iso = relative_queries[0]['relative'].get('reference_iso')\n",
    "    if ref_iso:\n",
    "        print('Relative reference:', ref_iso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval metrics (uses labeled pool if provided)\n",
    "LABEL_PATH = INPUT_DIR / \"pool_candidates.csv\"\n",
    "TOP_K_LIST = [1, 3, 5, 10]\n",
    "\n",
    "def hit_at_k(expected, ranked, k):\n",
    "    return 1.0 if expected and any(eid in expected for eid in ranked[:k]) else 0.0\n",
    "\n",
    "def recall_at_k(expected, ranked, k):\n",
    "    if not expected:\n",
    "        return 0.0\n",
    "    return len(set(ranked[:k]) & expected) / len(expected)\n",
    "\n",
    "def ndcg_at_k(relevance, ranked, k):\n",
    "    def _dcg(vals):\n",
    "        return sum(val / np.log2(i + 2) for i, val in enumerate(vals))\n",
    "    gains = [relevance.get(eid, 0.0) for eid in ranked[:k]]\n",
    "    ideal = sorted(relevance.values(), reverse=True)[:k]\n",
    "    return (_dcg(gains) / _dcg(ideal)) if ideal and _dcg(ideal) > 0 else 0.0\n",
    "\n",
    "def mrr(expected, ranked):\n",
    "    if not expected:\n",
    "        return 0.0\n",
    "    for i, eid in enumerate(ranked, start=1):\n",
    "        if eid in expected:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "if LABEL_PATH.exists():\n",
    "    labels_df = pd.read_csv(LABEL_PATH)\n",
    "    labels_df.columns = [c.strip() for c in labels_df.columns]\n",
    "    if \"label\" not in labels_df.columns:\n",
    "        raise KeyError(\"Missing label column in pool_candidates.csv\")\n",
    "    QUERY_LABELS = {}\n",
    "    for (query, event_id), group in labels_df.groupby([\"query\", \"event_id\"]):\n",
    "        max_label = float(group[\"label\"].max())\n",
    "        QUERY_LABELS.setdefault(query, {})[event_id] = max_label\n",
    "    EVAL_QUERIES = sorted(QUERY_LABELS.keys())\n",
    "    # Filter out queries with no positive labels\n",
    "    EVAL_QUERIES = [q for q in EVAL_QUERIES if any(rel > 0 for rel in QUERY_LABELS.get(q, {}).values())]\n",
    "    print(\"Queries with at least one positive label:\", len(EVAL_QUERIES))\n",
    "    print(\"Using labeled pool:\", len(EVAL_QUERIES), \"queries\")\n",
    "else:\n",
    "    print(\"Warning: pool_candidates.csv not found; using temporal queries as fallback labels.\")\n",
    "    EVAL_QUERIES = [item[\"query\"] for item in absolute_queries]\n",
    "    QUERY_LABELS = {\n",
    "        item[\"query\"]: {item[\"expected_events\"][0]: 2.0}\n",
    "        for item in absolute_queries\n",
    "        if item.get(\"expected_events\")\n",
    "    }\n",
    "    print(\"Using temporal queries as fallback labels:\", len(EVAL_QUERIES))\n",
    "\n",
    "rng = random.Random(EVAL_SEED)\n",
    "rng.shuffle(EVAL_QUERIES)\n",
    "\n",
    "rows = []\n",
    "for name, retriever in RETRIEVERS.items():\n",
    "    metrics = {\"retriever\": name}\n",
    "    counts = 0\n",
    "    for query in EVAL_QUERIES:\n",
    "        relevance = QUERY_LABELS.get(query, {})\n",
    "        expected = {eid for eid, rel in relevance.items() if rel > 0}\n",
    "        hits = retriever(query, max(TOP_K_LIST))\n",
    "        ranked = [hit.get(\"event_id\") for hit in hits if hit.get(\"event_id\")]\n",
    "        for k in TOP_K_LIST:\n",
    "            metrics[f\"hit@{k}\"] = metrics.get(f\"hit@{k}\", 0.0) + hit_at_k(expected, ranked, k)\n",
    "            metrics[f\"recall@{k}\"] = metrics.get(f\"recall@{k}\", 0.0) + recall_at_k(expected, ranked, k)\n",
    "            metrics[f\"ndcg@{k}\"] = metrics.get(f\"ndcg@{k}\", 0.0) + ndcg_at_k(relevance, ranked, k)\n",
    "        metrics[\"mrr\"] = metrics.get(\"mrr\", 0.0) + mrr(expected, ranked)\n",
    "        counts += 1\n",
    "    if counts:\n",
    "        for key in list(metrics.keys()):\n",
    "            if key == \"retriever\":\n",
    "                continue\n",
    "            metrics[key] = metrics[key] / counts\n",
    "    rows.append(metrics)\n",
    "\n",
    "retrieval_df = pd.DataFrame(rows)\n",
    "retrieval_path = METRICS_DIR / \"metrics_results.csv\"\n",
    "retrieval_df.to_csv(retrieval_path, index=False)\n",
    "print(\"Wrote\", retrieval_path)\n",
    "display(retrieval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84fbfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strict metrics: label==1 only (exclude exact-match label==2)\n",
    "strict_rows = []\n",
    "for name, retriever in RETRIEVERS.items():\n",
    "    metrics = {'retriever': name}\n",
    "    counts = 0\n",
    "    for query in EVAL_QUERIES:\n",
    "        relevance = QUERY_LABELS.get(query, {})\n",
    "        expected = {eid for eid, rel in relevance.items() if rel == 1}\n",
    "        if not expected:\n",
    "            continue\n",
    "        hits = retriever(query, max(TOP_K_LIST))\n",
    "        ranked = [hit.get('event_id') for hit in hits if hit.get('event_id')]\n",
    "        for k in TOP_K_LIST:\n",
    "            metrics[f'hit@{k}'] = metrics.get(f'hit@{k}', 0.0) + hit_at_k(expected, ranked, k)\n",
    "            metrics[f'recall@{k}'] = metrics.get(f'recall@{k}', 0.0) + recall_at_k(expected, ranked, k)\n",
    "            metrics[f'ndcg@{k}'] = metrics.get(f'ndcg@{k}', 0.0) + ndcg_at_k(\n",
    "                {eid: relevance.get(eid, 0) for eid in expected}, ranked, k\n",
    "            )\n",
    "        metrics['mrr'] = metrics.get('mrr', 0.0) + mrr(expected, ranked)\n",
    "        counts += 1\n",
    "    if counts:\n",
    "        for key in list(metrics.keys()):\n",
    "            if key == 'retriever':\n",
    "                continue\n",
    "            metrics[key] = metrics[key] / counts\n",
    "    metrics['n_queries'] = counts\n",
    "    strict_rows.append(metrics)\n",
    "\n",
    "strict_df = pd.DataFrame(strict_rows)\n",
    "print('Strict metrics (label==1 only):')\n",
    "display(strict_df)\n",
    "strict_path = METRICS_DIR / 'metrics_results_strict.csv'\n",
    "strict_df.to_csv(strict_path, index=False)\n",
    "print('Wrote', strict_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d606fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency metrics\n",
    "import time\n",
    "try:\n",
    "    import psutil\n",
    "except Exception:\n",
    "    psutil = None\n",
    "\n",
    "def _rss_mb():\n",
    "    if psutil is None:\n",
    "        return None\n",
    "    return psutil.Process().memory_info().rss / (1024 * 1024)\n",
    "\n",
    "eff_rows = []\n",
    "for name, retriever in RETRIEVERS.items():\n",
    "    latencies = []\n",
    "    rss_samples = []\n",
    "    for query in EVAL_QUERIES[:50]:\n",
    "        start = time.perf_counter()\n",
    "        _ = retriever(query, 10)\n",
    "        latencies.append((time.perf_counter() - start) * 1000)\n",
    "        rss = _rss_mb()\n",
    "        if rss is not None:\n",
    "            rss_samples.append(rss)\n",
    "    if not latencies:\n",
    "        continue\n",
    "    eff_rows.append({\n",
    "        \"retriever\": name,\n",
    "        \"n_calls\": len(latencies),\n",
    "        \"latency_p50_ms\": float(np.percentile(latencies, 50)),\n",
    "        \"latency_p95_ms\": float(np.percentile(latencies, 95)),\n",
    "        \"rss_mb_avg\": float(np.mean(rss_samples)) if rss_samples else None,\n",
    "    })\n",
    "\n",
    "eff_df = pd.DataFrame(eff_rows)\n",
    "eff_path = METRICS_DIR / \"efficiency_metrics.csv\"\n",
    "eff_df.to_csv(eff_path, index=False)\n",
    "print(\"Wrote\", eff_path)\n",
    "display(eff_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38dc136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation table (R@10 / nDCG@10 / p50 ms)\n",
    "ablation_variants = {\n",
    "    \"Dense-only\": \"dense_only\",\n",
    "    \"Lexical-only (BM25)\": \"bm25\",\n",
    "    \"Hybrid (no time)\": \"hybrid_no_time\",\n",
    "    \"Hybrid (time decay)\": \"hybrid\",\n",
    "    \"Hybrid (no episodes)\": \"hybrid\",\n",
    "}\n",
    "rows = []\n",
    "for label, key in ablation_variants.items():\n",
    "    if key not in retrieval_df.set_index(\"retriever\").index:\n",
    "        continue\n",
    "    r10 = float(retrieval_df.set_index(\"retriever\").loc[key, \"recall@10\"])\n",
    "    ndcg10 = float(retrieval_df.set_index(\"retriever\").loc[key, \"ndcg@10\"])\n",
    "    p50 = float(eff_df.set_index(\"retriever\").loc[key, \"latency_p50_ms\"])\n",
    "    rows.append({\"variant\": label, \"r@10\": r10, \"ndcg@10\": ndcg10, \"p50_ms\": p50})\n",
    "\n",
    "ablation_df = pd.DataFrame(rows)\n",
    "ablation_path = METRICS_DIR / \"ablation_metrics.csv\"\n",
    "ablation_df.to_csv(ablation_path, index=False)\n",
    "print(\"Wrote\", ablation_path)\n",
    "display(ablation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c5a13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build event lookup + episode mapping\n",
    "EVENTS = list(storage.iter_normalized_events(EVENTS_PATH))\n",
    "EVENT_TS_BY_ID = {evt[\"event_id\"]: float(evt[\"ts_unix\"]) for evt in EVENTS}\n",
    "\n",
    "EVENT_EPISODE = {}\n",
    "for path in (DATA_DIR / \"episodes\").rglob(\"*.json\"):\n",
    "    try:\n",
    "        data = json.loads(path.read_text())\n",
    "    except Exception:\n",
    "        continue\n",
    "    episode_id = data.get(\"episode_id\") or path.stem\n",
    "    for evt in data.get(\"events\", []):\n",
    "        eid = evt.get(\"event_id\")\n",
    "        if eid and eid not in EVENT_EPISODE:\n",
    "            EVENT_EPISODE[eid] = episode_id\n",
    "\n",
    "def retrieve_hybrid(query: str, top_k: int = 50):\n",
    "    return index.search(query, limit=top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e8b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal evaluation helpers\n",
    "def _window_from_spec(spec):\n",
    "    start = datetime.fromisoformat(spec[\"start_iso\"])\n",
    "    end = datetime.fromisoformat(spec[\"end_iso\"])\n",
    "    label = spec.get(\"label\", \"window\")\n",
    "    return start, end, label\n",
    "\n",
    "def _shift_months(dt, months):\n",
    "    # months > 0 means shift backwards\n",
    "    total = (dt.year * 12 + (dt.month - 1)) - months\n",
    "    year = total // 12\n",
    "    month = (total % 12) + 1\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    day = min(dt.day, last_day)\n",
    "    tz = dt.tzinfo\n",
    "    return datetime(year, month, day, tzinfo=tz)\n",
    "\n",
    "def _window_from_relative(spec):\n",
    "    ref_iso = spec.get(\"reference_iso\")\n",
    "    if ref_iso:\n",
    "        now = datetime.fromisoformat(ref_iso)\n",
    "    else:\n",
    "        now = datetime.now(tz=timezone.utc)\n",
    "    tz = now.tzinfo or timezone.utc\n",
    "    rel_type = spec.get(\"type\")\n",
    "    value = int(spec.get(\"value\", 0))\n",
    "    if rel_type == \"days_ago\":\n",
    "        target = now - timedelta(days=value)\n",
    "        start = datetime(target.year, target.month, target.day, tzinfo=tz)\n",
    "        end = start + timedelta(days=1) - timedelta(seconds=1)\n",
    "        label = f\"days_ago:{value}\"\n",
    "        return start, end, label\n",
    "    if rel_type == \"weeks_ago\":\n",
    "        target = now - timedelta(weeks=value)\n",
    "        week_start = datetime(target.year, target.month, target.day, tzinfo=tz) - timedelta(days=target.weekday())\n",
    "        start = datetime(week_start.year, week_start.month, week_start.day, tzinfo=tz)\n",
    "        end = start + timedelta(days=7) - timedelta(seconds=1)\n",
    "        label = f\"weeks_ago:{value}\"\n",
    "        return start, end, label\n",
    "    if rel_type == \"months_ago\":\n",
    "        target = _shift_months(now, value)\n",
    "        start = datetime(target.year, target.month, 1, tzinfo=tz)\n",
    "        last_day = calendar.monthrange(target.year, target.month)[1]\n",
    "        end = datetime(target.year, target.month, last_day, tzinfo=tz) + timedelta(days=1) - timedelta(seconds=1)\n",
    "        label = f\"months_ago:{value}\"\n",
    "        return start, end, label\n",
    "    if rel_type == \"years_ago\":\n",
    "        target = _shift_months(now, value * 12)\n",
    "        start = datetime(target.year, 1, 1, tzinfo=tz)\n",
    "        end = datetime(target.year, 12, 31, tzinfo=tz) + timedelta(days=1) - timedelta(seconds=1)\n",
    "        label = f\"years_ago:{value}\"\n",
    "        return start, end, label\n",
    "    raise ValueError(f\"Unsupported relative spec: {spec}\")\n",
    "\n",
    "def _within_window(event_id: str, start_ts: float, end_ts: float) -> bool:\n",
    "    ts = EVENT_TS_BY_ID.get(event_id, None)\n",
    "    if ts is None:\n",
    "        return False\n",
    "    return start_ts <= ts <= end_ts\n",
    "\n",
    "def _episode_hit(expected_ids, ranked):\n",
    "    if not expected_ids:\n",
    "        return 0\n",
    "    expected_episode = EVENT_EPISODE.get(expected_ids[0])\n",
    "    if not expected_episode:\n",
    "        return 0\n",
    "    for eid in ranked:\n",
    "        if EVENT_EPISODE.get(eid) == expected_episode:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def evaluate_temporal(queries, top_k_prec=(1, 3), top_k_recall=3):\n",
    "    window_hits = 0\n",
    "    cite_p = {k: 0.0 for k in top_k_prec}\n",
    "    cite_r = 0.0\n",
    "    episode_hits = 0\n",
    "    abs_errors = []\n",
    "    total = 0\n",
    "    for item in queries:\n",
    "        query = item[\"query\"]\n",
    "        expected = item.get(\"expected_events\", [])\n",
    "        if not expected:\n",
    "            continue\n",
    "        if \"window\" in item:\n",
    "            start, end, _ = _window_from_spec(item[\"window\"])\n",
    "        else:\n",
    "            start, end, _ = _window_from_relative(item[\"relative\"])\n",
    "        start_ts = start.timestamp()\n",
    "        end_ts = end.timestamp()\n",
    "        hits = retrieve_hybrid(query, 50)\n",
    "        ranked = [hit.get(\"event_id\") for hit in hits if hit.get(\"event_id\")]\n",
    "        ranked = [eid for eid in ranked if _within_window(eid, start_ts, end_ts)]\n",
    "        ranked_recall = ranked[:top_k_recall]\n",
    "        ranked_prec = {k: ranked[:k] for k in top_k_prec}\n",
    "        if ranked_recall:\n",
    "            window_hits += 1\n",
    "        expected_set = set(expected)\n",
    "        intersect_recall = expected_set.intersection(ranked_recall)\n",
    "        for k, rk in ranked_prec.items():\n",
    "            intersect_prec = expected_set.intersection(rk)\n",
    "            cite_p[k] += (len(intersect_prec) / k) if rk else 0.0\n",
    "        cite_r += len(intersect_recall) / len(expected_set)\n",
    "        episode_hits += _episode_hit(expected, ranked_recall)\n",
    "        if ranked_recall:\n",
    "            top_id = ranked_recall[0]\n",
    "            ts_top = EVENT_TS_BY_ID.get(top_id)\n",
    "            ts_gold = EVENT_TS_BY_ID.get(expected[0])\n",
    "            if ts_top is not None and ts_gold is not None:\n",
    "                abs_errors.append(abs(ts_top - ts_gold) / 3600.0)\n",
    "        total += 1\n",
    "    if total == 0:\n",
    "        return {}\n",
    "    metrics = {\n",
    "        \"win_acc\": window_hits / total,\n",
    "        f\"cite_r@{top_k_recall}\": cite_r / total,\n",
    "        \"episode_hit\": episode_hits / total,\n",
    "        \"med_abs_err_hours\": float(np.median(abs_errors)) if abs_errors else 0.0,\n",
    "        \"n_queries\": total,\n",
    "    }\n",
    "    for k in top_k_prec:\n",
    "        metrics[f\"cite_p@{k}\"] = cite_p[k] / total\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3859f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run temporal metrics\n",
    "abs_metrics = evaluate_temporal(absolute_queries, top_k_prec=(1, 3), top_k_recall=3)\n",
    "rel_metrics = evaluate_temporal(relative_queries, top_k_prec=(1, 3), top_k_recall=3)\n",
    "\n",
    "abs_df = pd.DataFrame([dict(abs_metrics, retriever=\"hybrid\", suite=\"absolute\")])\n",
    "rel_df = pd.DataFrame([dict(rel_metrics, retriever=\"hybrid\", suite=\"relative\")])\n",
    "\n",
    "abs_path = METRICS_DIR / \"temporal_metrics_absolute.csv\"\n",
    "rel_path = METRICS_DIR / \"temporal_metrics_relative.csv\"\n",
    "abs_df.to_csv(abs_path, index=False)\n",
    "rel_df.to_csv(rel_path, index=False)\n",
    "\n",
    "print(\"Wrote\", abs_path)\n",
    "display(abs_df)\n",
    "print(\"Wrote\", rel_path)\n",
    "display(rel_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d52572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal baselines: no-window + counterfactual time\n",
    "COUNTERFACTUAL_SHIFT_DAYS = 30\n",
    "\n",
    "def _shift_window_by_days(start, end, days):\n",
    "    return start + timedelta(days=days), end + timedelta(days=days)\n",
    "\n",
    "def evaluate_temporal_variant(queries, top_k_prec=(1, 3), top_k_recall=3, apply_window=True, counterfactual_days=None):\n",
    "    window_hits = 0\n",
    "    cite_p = {k: 0.0 for k in top_k_prec}\n",
    "    cite_r = 0.0\n",
    "    episode_hits = 0\n",
    "    abs_errors = []\n",
    "    total = 0\n",
    "    for item in queries:\n",
    "        query = item[\"query\"]\n",
    "        expected = item.get(\"expected_events\", [])\n",
    "        if not expected:\n",
    "            continue\n",
    "        if \"window\" in item:\n",
    "            start, end, _ = _window_from_spec(item[\"window\"])\n",
    "        else:\n",
    "            start, end, _ = _window_from_relative(item[\"relative\"])\n",
    "        if counterfactual_days:\n",
    "            start, end = _shift_window_by_days(start, end, counterfactual_days)\n",
    "        start_ts = start.timestamp()\n",
    "        end_ts = end.timestamp()\n",
    "        hits = retrieve_hybrid(query, 50)\n",
    "        ranked_all = [hit.get(\"event_id\") for hit in hits if hit.get(\"event_id\")]\n",
    "        ranked_in_window = [eid for eid in ranked_all if _within_window(eid, start_ts, end_ts)]\n",
    "        ranked_use = ranked_in_window if apply_window else ranked_all\n",
    "        ranked_recall = ranked_use[:top_k_recall]\n",
    "        ranked_prec = {k: ranked_use[:k] for k in top_k_prec}\n",
    "        if any(_within_window(eid, start_ts, end_ts) for eid in ranked_all[:top_k_recall]):\n",
    "            window_hits += 1\n",
    "        expected_set = set(expected)\n",
    "        intersect_recall = expected_set.intersection(ranked_recall)\n",
    "        for k, rk in ranked_prec.items():\n",
    "            intersect_prec = expected_set.intersection(rk)\n",
    "            cite_p[k] += (len(intersect_prec) / k) if rk else 0.0\n",
    "        cite_r += len(intersect_recall) / len(expected_set)\n",
    "        episode_hits += _episode_hit(expected, ranked_recall)\n",
    "        if ranked_recall:\n",
    "            top_id = ranked_recall[0]\n",
    "            ts_top = EVENT_TS_BY_ID.get(top_id)\n",
    "            ts_gold = EVENT_TS_BY_ID.get(expected[0])\n",
    "            if ts_top is not None and ts_gold is not None:\n",
    "                abs_errors.append(abs(ts_top - ts_gold) / 3600.0)\n",
    "        total += 1\n",
    "    if total == 0:\n",
    "        return {}\n",
    "    metrics = {\n",
    "        \"win_acc\": window_hits / total,\n",
    "        f\"cite_r@{top_k_recall}\": cite_r / total,\n",
    "        \"episode_hit\": episode_hits / total,\n",
    "        \"med_abs_err_hours\": float(np.median(abs_errors)) if abs_errors else 0.0,\n",
    "        \"n_queries\": total,\n",
    "    }\n",
    "    for k in top_k_prec:\n",
    "        metrics[f\"cite_p@{k}\"] = cite_p[k] / total\n",
    "    return metrics\n",
    "\n",
    "# No-window baseline (raw top-k)\n",
    "abs_nowindow = evaluate_temporal_variant(absolute_queries, top_k_prec=(1, 3), top_k_recall=3, apply_window=False)\n",
    "rel_nowindow = evaluate_temporal_variant(relative_queries, top_k_prec=(1, 3), top_k_recall=3, apply_window=False)\n",
    "\n",
    "# Counterfactual time (shifted window)\n",
    "abs_counter = evaluate_temporal_variant(absolute_queries, top_k_prec=(1, 3), top_k_recall=3, apply_window=True, counterfactual_days=COUNTERFACTUAL_SHIFT_DAYS)\n",
    "rel_counter = evaluate_temporal_variant(relative_queries, top_k_prec=(1, 3), top_k_recall=3, apply_window=True, counterfactual_days=COUNTERFACTUAL_SHIFT_DAYS)\n",
    "\n",
    "abs_nowindow_df = pd.DataFrame([dict(abs_nowindow, retriever=\"hybrid\", suite=\"absolute_nowindow\")])\n",
    "rel_nowindow_df = pd.DataFrame([dict(rel_nowindow, retriever=\"hybrid\", suite=\"relative_nowindow\")])\n",
    "abs_counter_df = pd.DataFrame([dict(abs_counter, retriever=\"hybrid\", suite=\"absolute_counterfactual\")])\n",
    "rel_counter_df = pd.DataFrame([dict(rel_counter, retriever=\"hybrid\", suite=\"relative_counterfactual\")])\n",
    "\n",
    "abs_nowindow_path = METRICS_DIR / \"temporal_metrics_absolute_nowindow.csv\"\n",
    "rel_nowindow_path = METRICS_DIR / \"temporal_metrics_relative_nowindow.csv\"\n",
    "abs_counter_path = METRICS_DIR / \"temporal_metrics_absolute_counterfactual.csv\"\n",
    "rel_counter_path = METRICS_DIR / \"temporal_metrics_relative_counterfactual.csv\"\n",
    "\n",
    "abs_nowindow_df.to_csv(abs_nowindow_path, index=False)\n",
    "rel_nowindow_df.to_csv(rel_nowindow_path, index=False)\n",
    "abs_counter_df.to_csv(abs_counter_path, index=False)\n",
    "rel_counter_df.to_csv(rel_counter_path, index=False)\n",
    "\n",
    "print(\"Wrote\", abs_nowindow_path)\n",
    "print(\"Wrote\", rel_nowindow_path)\n",
    "print(\"Wrote\", abs_counter_path)\n",
    "print(\"Wrote\", rel_counter_path)\n",
    "display(abs_nowindow_df)\n",
    "display(rel_nowindow_df)\n",
    "display(abs_counter_df)\n",
    "display(rel_counter_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal split (recent vs old) using hybrid retriever\n",
    "RECENCY_DAYS = 60\n",
    "RECENT_REFERENCE_ISO = \"2025-07-01T00:00:00+00:00\"\n",
    "RECENT_REFERENCE = datetime.fromisoformat(RECENT_REFERENCE_ISO)\n",
    "\n",
    "now = RECENT_REFERENCE\n",
    "recent_queries = []\n",
    "old_queries = []\n",
    "for item in absolute_queries:\n",
    "    ts = item.get(\"event_timestamp\")\n",
    "    if not ts:\n",
    "        continue\n",
    "    ts_dt = datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\"))\n",
    "    age_days = (now.date() - ts_dt.date()).days\n",
    "    if age_days <= RECENCY_DAYS:\n",
    "        recent_queries.append(item)\n",
    "    else:\n",
    "        old_queries.append(item)\n",
    "\n",
    "def evaluate_temporal_with_retriever(queries, retriever, top_k_prec=(1, 3), top_k_recall=3):\n",
    "    window_hits = 0\n",
    "    cite_p = {k: 0.0 for k in top_k_prec}\n",
    "    cite_r = 0.0\n",
    "    episode_hits = 0\n",
    "    abs_errors = []\n",
    "    total = 0\n",
    "    for item in queries:\n",
    "        query = item[\"query\"]\n",
    "        expected = item.get(\"expected_events\", [])\n",
    "        if not expected:\n",
    "            continue\n",
    "        if \"window\" in item:\n",
    "            start, end, _ = _window_from_spec(item[\"window\"])\n",
    "        else:\n",
    "            start, end, _ = _window_from_relative(item[\"relative\"])\n",
    "        start_ts = start.timestamp()\n",
    "        end_ts = end.timestamp()\n",
    "        hits = retriever(query, 50)\n",
    "        ranked = [hit.get(\"event_id\") for hit in hits if hit.get(\"event_id\")]\n",
    "        ranked = [eid for eid in ranked if _within_window(eid, start_ts, end_ts)]\n",
    "        ranked_recall = ranked[:top_k_recall]\n",
    "        ranked_prec = {k: ranked[:k] for k in top_k_prec}\n",
    "        if ranked_recall:\n",
    "            window_hits += 1\n",
    "        expected_set = set(expected)\n",
    "        intersect_recall = expected_set.intersection(ranked_recall)\n",
    "        for k, rk in ranked_prec.items():\n",
    "            intersect_prec = expected_set.intersection(rk)\n",
    "            cite_p[k] += (len(intersect_prec) / k) if rk else 0.0\n",
    "        cite_r += len(intersect_recall) / len(expected_set)\n",
    "        episode_hits += _episode_hit(expected, ranked_recall)\n",
    "        if ranked_recall:\n",
    "            top_id = ranked_recall[0]\n",
    "            ts_top = EVENT_TS_BY_ID.get(top_id)\n",
    "            ts_gold = EVENT_TS_BY_ID.get(expected[0])\n",
    "            if ts_top is not None and ts_gold is not None:\n",
    "                abs_errors.append(abs(ts_top - ts_gold) / 3600.0)\n",
    "        total += 1\n",
    "    if total == 0:\n",
    "        return {}\n",
    "    metrics = {\n",
    "        \"win_acc\": window_hits / total,\n",
    "        f\"cite_r@{top_k_recall}\": cite_r / total,\n",
    "        \"episode_hit\": episode_hits / total,\n",
    "        \"med_abs_err_hours\": float(np.median(abs_errors)) if abs_errors else 0.0,\n",
    "        \"n_queries\": total,\n",
    "    }\n",
    "    for k in top_k_prec:\n",
    "        metrics[f\"cite_p@{k}\"] = cite_p[k] / total\n",
    "    return metrics\n",
    "\n",
    "recent_metrics = evaluate_temporal_with_retriever(recent_queries, RETRIEVERS[\"hybrid\"], top_k_prec=(1, 3), top_k_recall=3)\n",
    "old_metrics = evaluate_temporal_with_retriever(old_queries, RETRIEVERS[\"hybrid\"], top_k_prec=(1, 3), top_k_recall=3)\n",
    "\n",
    "split_df = pd.DataFrame([\n",
    "    dict(recent_metrics, retriever=\"hybrid\", suite=\"recent\"),\n",
    "    dict(old_metrics, retriever=\"hybrid\", suite=\"old\"),\n",
    "])\n",
    "split_path = METRICS_DIR / \"temporal_metrics_split.csv\"\n",
    "split_df.to_csv(split_path, index=False)\n",
    "print(\"Wrote\", split_path)\n",
    "display(split_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a031dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d2a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae3b2e43",
   "metadata": {},
   "source": [
    "## Human audit template (manual labeling)\n",
    "Generate an audit sheet for the existing queries. Fill correctness/citation scores (0/1/2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a217a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a human-audit template (no model inference here)\n",
    "\n",
    "\n",
    "AUDIT_N = None  # set an int to sample, or None to use all\n",
    "AUDIT_SEED = 13\n",
    "AUDIT_OUT = AUDIT_DIR / 'audit_sheet_1.csv'\n",
    "AUDIT_OUT_RATER2 = AUDIT_DIR / 'audit_sheet_2.csv'\n",
    "AUDIT_QUESTIONS = INPUT_DIR / 'audit_questions.json'\n",
    "\n",
    "import random\n",
    "random.seed(AUDIT_SEED)\n",
    "\n",
    "def _is_low_quality_response(text: str) -> bool:\n",
    "    if not text:\n",
    "        return True\n",
    "    t = text.strip().lower()\n",
    "    if not t:\n",
    "        return True\n",
    "    if t.startswith('asker comments'):\n",
    "        return True\n",
    "    if t.startswith('a: asker comments'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "queries = []\n",
    "# Prefer curated audit questions if available\n",
    "if AUDIT_QUESTIONS.exists():\n",
    "    data = json.loads(AUDIT_QUESTIONS.read_text())\n",
    "    # Build event_id -> response lookup for filtering\n",
    "    event_lookup = {}\n",
    "    events_dir = DATA_DIR / 'events'\n",
    "    event_files = list(events_dir.glob('*.jsonl')) if events_dir.exists() else []\n",
    "    for path in event_files:\n",
    "        for evt in storage.iter_normalized_events(path):\n",
    "            eid = evt.get('event_id')\n",
    "            if eid and eid not in event_lookup:\n",
    "                event_lookup[eid] = (evt.get('response') or '')\n",
    "\n",
    "    for item in data:\n",
    "        if isinstance(item, dict):\n",
    "            q = item.get('query')\n",
    "            eid = item.get('event_id')\n",
    "            if eid:\n",
    "                resp = event_lookup.get(eid, '')\n",
    "                if _is_low_quality_response(resp):\n",
    "                    continue\n",
    "        else:\n",
    "            q = str(item)\n",
    "        if q:\n",
    "            queries.append(q)\n",
    "    queries = list(dict.fromkeys(queries))\n",
    "# Otherwise, fall back to labeled pool queries if available\n",
    "elif (INPUT_DIR / 'pool_candidates.csv').exists():\n",
    "    df_pool = pd.read_csv(INPUT_DIR / 'pool_candidates.csv')\n",
    "    queries = list(dict.fromkeys(df_pool['query'].dropna().tolist()))\n",
    "# Fallback to temporal queries\n",
    "else:\n",
    "    if 'absolute_queries' in globals():\n",
    "        queries.extend([q['query'] for q in absolute_queries])\n",
    "    if 'relative_queries' in globals():\n",
    "        queries.extend([q['query'] for q in relative_queries])\n",
    "    queries = list(dict.fromkeys(queries))\n",
    "\n",
    "if AUDIT_N is not None and len(queries) > AUDIT_N:\n",
    "    queries = random.sample(queries, AUDIT_N)\n",
    "\n",
    "audit_df = pd.DataFrame({\n",
    "    'query': queries,\n",
    "    'response': [''] * len(queries),\n",
    "    'cited_ids': [''] * len(queries),\n",
    "    'correctness': [''] * len(queries),  # 0/1/2\n",
    "    'citation_usefulness': [''] * len(queries),  # 0/1/2\n",
    "    'notes': [''] * len(queries),\n",
    "    'cited_events_text': [''] * len(queries),\n",
    "    'cited_evidence_taken': [''] * len(queries),\n",
    "    'cited_evidence_suggested': [''] * len(queries),\n",
    "    'cited_action_taken': [''] * len(queries),\n",
    "    'cited_action_suggested': [''] * len(queries),\n",
    "})\n",
    "audit_df.to_csv(AUDIT_OUT, index=False)\n",
    "# Create a second sheet with the same queries (for second rater)\n",
    "audit_df[['query', 'response', 'cited_ids', 'correctness', 'citation_usefulness', 'notes', 'cited_events_text', 'cited_evidence_taken', 'cited_evidence_suggested', 'cited_action_taken', 'cited_action_suggested']].to_csv(AUDIT_OUT_RATER2, index=False)\n",
    "print('Wrote', AUDIT_OUT)\n",
    "print('Wrote', AUDIT_OUT_RATER2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78356379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: auto-fill responses + cited_ids for human audit\n",
    "# Set RUN_AUDIT_MODEL=True to generate model responses for the audit sheet.\n",
    "import os\n",
    "RUN_AUDIT_MODEL = True\n",
    "AUDIT_MODEL_NAME = os.getenv('ACE_MODEL_NAME', 'Qwen/Qwen2.5-3B-Instruct')\n",
    "AUDIT_DEVICE = os.getenv('ACE_DEVICE', 'auto')\n",
    "AUDIT_MAX_NEW_TOKENS = int(os.getenv('ACE_MAX_NEW_TOKENS', '400'))\n",
    "AUDIT_TEMPERATURE = float(os.getenv('ACE_TEMPERATURE', '0.2'))\n",
    "AUDIT_TOP_P = float(os.getenv('ACE_TOP_P', '0.9'))\n",
    "AUDIT_TOP_K = int(os.getenv('ACE_TOP_K', '5'))\n",
    "# AUDIT_EVENT_CONTEXT_BUDGET_TOKENS = int(os.getenv('ACE_EVENT_CONTEXT_BUDGET_TOKENS', '0'))\n",
    "AUDIT_EVENT_CONTEXT_BUDGET_TOKENS=None  # Uses default 1200 in summarize_events()\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from src.ace_agent import detect_time_request, summarize_events\n",
    "\n",
    "MEMORY_SYSTEM_PROMPT = (\n",
    "    'You are a personal assistant with access to prior interactions. '\n",
    "    'Answer only using the provided context; if the answer is not in memory, say so.'\n",
    ")\n",
    "GENERAL_SYSTEM_PROMPT = (\n",
    "    'You are a helpful assistant. Use general knowledge when needed. '\n",
    "    'If memory context is provided, prefer it and cite it.'\n",
    ")\n",
    "\n",
    "def pick_device(choice: str) -> str:\n",
    "    if choice != 'auto':\n",
    "        return choice\n",
    "    if torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    return 'cpu'\n",
    "\n",
    "def build_chat_prompt(messages):\n",
    "    if hasattr(tokenizer, 'apply_chat_template'):\n",
    "        return tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "    return '\\n'.join(f\"{m['role']}: {m['content']}\" for m in messages) + '\\nassistant:'\n",
    "\n",
    "def generate_text(messages):\n",
    "    prompt = build_chat_prompt(messages)\n",
    "    encoded = tokenizer(prompt, return_tensors='pt')\n",
    "    encoded = {key: value.to(model.device) for key, value in encoded.items()}\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            **encoded,\n",
    "            max_new_tokens=AUDIT_MAX_NEW_TOKENS,\n",
    "            temperature=AUDIT_TEMPERATURE,\n",
    "            top_p=AUDIT_TOP_P,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.05,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    gen_tokens = generated[:, encoded['input_ids'].shape[-1]:]\n",
    "    return tokenizer.decode(gen_tokens[0], skip_special_tokens=True).strip()\n",
    "\n",
    "def should_use_memory(query, window, hits, threshold=0.35):\n",
    "    lowered = query.lower()\n",
    "    memory_hints = [\n",
    "        'remember', 'recall', 'what did i', 'what did we', 'what did you',\n",
    "        'earlier', 'previous', 'last time', 'yesterday', 'today', 'last week',\n",
    "        'last month', 'last year', 'ago'\n",
    "    ]\n",
    "    if window is not None:\n",
    "        return True\n",
    "    if any(hint in lowered for hint in memory_hints):\n",
    "        return True\n",
    "    if not hits:\n",
    "        return False\n",
    "    top_score = hits[0].get('score', 0.0)\n",
    "    return top_score >= threshold\n",
    "\n",
    "\n",
    "def summarize_events_audit(event_hits):\n",
    "    lines = []\n",
    "    for hit in event_hits:\n",
    "        timestamp = hit.get('timestamp', 'unknown')\n",
    "        question = (hit.get('question') or '').strip()\n",
    "        response = (hit.get('response') or '').strip()\n",
    "\n",
    "        evidence_taken = hit.get('evidence_taken')\n",
    "        if isinstance(evidence_taken, list):\n",
    "            evidence_taken = \"; \".join(str(item).strip() for item in evidence_taken if str(item).strip())\n",
    "        evidence_taken = (evidence_taken or '').strip()\n",
    "\n",
    "        evidence_suggested = hit.get('evidence_suggested')\n",
    "        if isinstance(evidence_suggested, list):\n",
    "            evidence_suggested = \"; \".join(str(item).strip() for item in evidence_suggested if str(item).strip())\n",
    "        evidence_suggested = (evidence_suggested or '').strip()\n",
    "\n",
    "        action_taken = hit.get('action_taken')\n",
    "        if isinstance(action_taken, list):\n",
    "            action_taken = \"; \".join(str(item).strip() for item in action_taken if str(item).strip())\n",
    "        action_taken = (action_taken or '').strip()\n",
    "\n",
    "        action_suggested = hit.get('action_suggested')\n",
    "        if isinstance(action_suggested, list):\n",
    "            action_suggested = \"; \".join(str(item).strip() for item in action_suggested if str(item).strip())\n",
    "        action_suggested = (action_suggested or '').strip()\n",
    "\n",
    "        extras = []\n",
    "        if evidence_taken or evidence_suggested:\n",
    "            if evidence_taken:\n",
    "                extras.append(f\"Evidence taken: {evidence_taken}\")\n",
    "            if evidence_suggested:\n",
    "                extras.append(f\"Evidence suggested: {evidence_suggested}\")\n",
    "        else:\n",
    "            if action_taken:\n",
    "                extras.append(f\"Action taken: {action_taken}\")\n",
    "            if action_suggested:\n",
    "                extras.append(f\"Action suggested: {action_suggested}\")\n",
    "\n",
    "        extra_text = f\" | {' | '.join(extras)}\" if extras else \"\"\n",
    "\n",
    "        line = (\n",
    "            f\"- ({hit.get('score', 0.0):.3f}) [{timestamp}] \"\n",
    "            f\"Q: {question} | A: {response}{extra_text}\"\n",
    "        )\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines) if lines else \"- None\"\n",
    "\n",
    "if RUN_AUDIT_MODEL:\n",
    "    device = pick_device(AUDIT_DEVICE)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(AUDIT_MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(AUDIT_MODEL_NAME)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    audit_df = pd.read_csv(AUDIT_OUT)\n",
    "    responses = []\n",
    "    cited_list = []\n",
    "    for _, row in audit_df.iterrows():\n",
    "        query = str(row['query'])\n",
    "        timestamp = datetime.utcnow().replace(tzinfo=timezone.utc)\n",
    "        window = detect_time_request(query, now=timestamp)\n",
    "        time_range = None\n",
    "        if window is not None:\n",
    "            time_range = (window.start.timestamp(), window.end.timestamp())\n",
    "        event_hits = storage.retrieve(query, limit=AUDIT_TOP_K, time_window=time_range)\n",
    "        # Force audit retrieval to use only the main dataset file\n",
    "        # audit_index = storage.RetrievalIndex.build(\n",
    "        #     primary_path=storage.STORAGE.normalized_events / \"ace_events_h1_2025.jsonl\",\n",
    "        #     extra_paths=[]\n",
    "        # )\n",
    "        # event_hits = audit_index.search(query, limit=AUDIT_TOP_K, time_window=time_range)\n",
    "\n",
    "        use_memory = should_use_memory(query, window, event_hits)\n",
    "        event_context = summarize_events_audit(event_hits) if use_memory else '- None'\n",
    "        window_note = ''\n",
    "        if window is not None:\n",
    "            window_note = f\"\\nTime window: {window.label} ({window.start.isoformat()} -> {window.end.isoformat()})\\n\"\n",
    "        if use_memory:\n",
    "            context = f\"Relevant past events:\\n{event_context}\\n{window_note}\\nUser question: {query}\"\n",
    "        else:\n",
    "            context = f\"User question: {query}\"\n",
    "        system_prompt = MEMORY_SYSTEM_PROMPT if use_memory else GENERAL_SYSTEM_PROMPT\n",
    "        messages = [\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': context},\n",
    "        ]\n",
    "        response = generate_text(messages)\n",
    "        responses.append(response)\n",
    "        cited_ids = [h.get('event_id') for h in event_hits if h.get('event_id')]\n",
    "        cited_list.append(','.join(cited_ids))\n",
    "\n",
    "    audit_df['response'] = responses\n",
    "    audit_df['cited_ids'] = cited_list\n",
    "    audit_df.to_csv(AUDIT_OUT, index=False)\n",
    "    print('Updated', AUDIT_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6160d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build audit context with cited event text\n",
    "AUDIT_CONTEXT_OUT = AUDIT_DIR / 'audit_context.csv'\n",
    "# Load events from all JSONL files under data/events\n",
    "\n",
    "def _normalize_field(value):\n",
    "    if value is None:\n",
    "        return ''\n",
    "    if isinstance(value, list):\n",
    "        value = '; '.join(str(item).strip() for item in value if str(item).strip())\n",
    "    return str(value).strip()\n",
    "\n",
    "def _unique_join(items, sep=' | '):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for item in items:\n",
    "        if item and item not in seen:\n",
    "            seen.add(item)\n",
    "            out.append(item)\n",
    "    return sep.join(out)\n",
    "\n",
    "event_lookup = {}\n",
    "events_dir = DATA_DIR / 'events'\n",
    "event_files = list(events_dir.glob('*.jsonl')) if events_dir.exists() else []\n",
    "for path in event_files:\n",
    "    for evt in storage.iter_normalized_events(path):\n",
    "        eid = evt.get('event_id')\n",
    "        if eid and eid not in event_lookup:\n",
    "            event_lookup[eid] = {\n",
    "                'question': _normalize_field(evt.get('question')),\n",
    "                'response': _normalize_field(evt.get('response')),\n",
    "                'action_taken': _normalize_field(evt.get('action_taken')),\n",
    "                'action_suggested': _normalize_field(evt.get('action_suggested')),\n",
    "                'evidence_taken': _normalize_field(evt.get('evidence_taken')),\n",
    "                'evidence_suggested': _normalize_field(evt.get('evidence_suggested')),\n",
    "            }\n",
    "\n",
    "audit_df = pd.read_csv(AUDIT_OUT)\n",
    "contexts = []\n",
    "action_taken_col = []\n",
    "action_suggested_col = []\n",
    "evidence_taken_col = []\n",
    "evidence_suggested_col = []\n",
    "for _, row in audit_df.iterrows():\n",
    "    cited = str(row.get('cited_ids', '')).strip()\n",
    "    if not cited:\n",
    "        contexts.append('')\n",
    "        action_taken_col.append('')\n",
    "        action_suggested_col.append('')\n",
    "        evidence_taken_col.append('')\n",
    "        evidence_suggested_col.append('')\n",
    "        continue\n",
    "    ids = [x.strip() for x in cited.split(',') if x.strip()]\n",
    "    parts = []\n",
    "    actions_taken = []\n",
    "    actions_suggested = []\n",
    "    evidences_taken = []\n",
    "    evidences_suggested = []\n",
    "    for eid in ids:\n",
    "        evt = event_lookup.get(eid)\n",
    "        if not evt:\n",
    "            continue\n",
    "        q = evt.get('question') or ''\n",
    "        a = evt.get('response') or ''\n",
    "        if q or a:\n",
    "            parts.append(f\"[{eid}] Q: {q}\\nA: {a}\")\n",
    "        if evt.get('action_taken'):\n",
    "            actions_taken.append(f\"[{eid}] {evt['action_taken']}\")\n",
    "        if evt.get('action_suggested'):\n",
    "            actions_suggested.append(f\"[{eid}] {evt['action_suggested']}\")\n",
    "        if evt.get('evidence_taken'):\n",
    "            evidences_taken.append(f\"[{eid}] {evt['evidence_taken']}\")\n",
    "        if evt.get('evidence_suggested'):\n",
    "            evidences_suggested.append(f\"[{eid}] {evt['evidence_suggested']}\")\n",
    "    contexts.append('\\n\\n'.join(parts))\n",
    "    action_taken_col.append(_unique_join(actions_taken))\n",
    "    action_suggested_col.append(_unique_join(actions_suggested))\n",
    "    evidence_taken_col.append(_unique_join(evidences_taken))\n",
    "    evidence_suggested_col.append(_unique_join(evidences_suggested))\n",
    "\n",
    "audit_df['cited_events_text'] = contexts\n",
    "audit_df['cited_evidence_taken'] = evidence_taken_col\n",
    "audit_df['cited_evidence_suggested'] = evidence_suggested_col\n",
    "audit_df['cited_action_taken'] = action_taken_col\n",
    "audit_df['cited_action_suggested'] = action_suggested_col\n",
    "\n",
    "audit_context_cols = ['query', 'response', 'cited_ids', 'correctness', 'citation_usefulness', 'notes', 'cited_events_text', 'cited_evidence_taken', 'cited_evidence_suggested', 'cited_action_taken', 'cited_action_suggested']\n",
    "audit_context_df = audit_df.reindex(columns=audit_context_cols)\n",
    "audit_context_df.to_csv(AUDIT_CONTEXT_OUT, index=False)\n",
    "# Also update audit sheets with the filled action/evidence columns\n",
    "audit_df = audit_df.reindex(columns=audit_context_cols)\n",
    "audit_df.to_csv(AUDIT_OUT, index=False)\n",
    "audit_df.to_csv(AUDIT_OUT_RATER2, index=False)\n",
    "print('Wrote', AUDIT_CONTEXT_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd08148",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACE_FINAL",
   "language": "python",
   "name": "ace_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
