{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACE Minimal RAG Agent\n",
    "\n",
    "This notebook runs a minimal storage-backed RAG loop. Supported temporal queries include:\n",
    "- yesterday / today / last week / last month / last year\n",
    "- N days/weeks/months/years ago\n",
    "- Jan 2025 / 2025-01 / 2025-03-31\n",
    "- from June 1, 2025 to June 15, 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a27e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT_CANDIDATE = Path.cwd().resolve()\n",
    "if (ROOT_CANDIDATE / 'ACE.ipynb').exists():\n",
    "    REPO_ROOT = ROOT_CANDIDATE\n",
    "elif (ROOT_CANDIDATE / 'ACE_release' / 'ACE.ipynb').exists():\n",
    "    REPO_ROOT = ROOT_CANDIDATE / 'ACE_release'\n",
    "else:\n",
    "    REPO_ROOT = ROOT_CANDIDATE\n",
    "DEFAULT_STORAGE_DIR = REPO_ROOT / 'data'\n",
    "EVENTS_PATH = DEFAULT_STORAGE_DIR / \"events\" / \"ace_events_h1_2025.jsonl\"\n",
    "EMBED_DIR = DEFAULT_STORAGE_DIR / \"embeddings\"\n",
    "EMBED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMBED_MODEL_NAME = \"intfloat/e5-large-v2\" # Used for current test/results\n",
    "# EMBED_MODEL_NAME = \"intfloat/e5-small-v2\" # smaller/faster for limited resources - results may vary\n",
    "EMBED_MODEL_SLUG = EMBED_MODEL_NAME.replace(\"/\", \"_\").replace(\"-\", \"_\")\n",
    "EMBED_PATH = EMBED_DIR / f\"event_embeddings_{EMBED_MODEL_SLUG}.npz\"\n",
    "EMBED_QUERY_PREFIX = \"query: \"\n",
    "EMBED_PASSAGE_PREFIX = \"passage: \"\n",
    "\n",
    "os.environ.setdefault('ACE_STORAGE_DIR', str(DEFAULT_STORAGE_DIR))\n",
    "# Optional override: os.environ['ACE_USER_TZ'] = 'America/New_York'\n",
    "\n",
    "sys.path.insert(0, str(REPO_ROOT / 'src'))\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "try:\n",
    "    from huggingface_hub import snapshot_download\n",
    "    from huggingface_hub.utils import LocalEntryNotFoundError\n",
    "except Exception:\n",
    "    snapshot_download = None\n",
    "    LocalEntryNotFoundError = Exception\n",
    "import src.storage_helpers as storage\n",
    "from src.ace_agent import run_chat_loop\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "REBUILD_EMBEDDINGS = False # Set to True force re-building embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddcb4be",
   "metadata": {},
   "source": [
    "## To Download and use the ACE dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ds = load_dataset(\"anon-user-423/ACE\", \"events\", split=\"train\")\n",
    "print(ds)\n",
    "print(\"Columns:\", ds.column_names)\n",
    "print(\"Sample:\", ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e7c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write events to JSONL (used by storage helpers)\n",
    "EVENTS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "if not EVENTS_PATH.exists():\n",
    "    with EVENTS_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for row in ds:\n",
    "            f.write(json.dumps(dict(row), ensure_ascii=True) + \"\\n\")\n",
    "    print(\"Wrote\", EVENTS_PATH)\n",
    "else:\n",
    "    print(\"Events file already exists:\", EVENTS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build episodes from dataset\n",
    "\n",
    "# ------ Will take around an hour or two to run ------\n",
    "\n",
    "episode_dir = DEFAULT_STORAGE_DIR / \"episodes\"\n",
    "if not episode_dir.exists() or not any(episode_dir.rglob(\"*.json\")):\n",
    "    summary = storage.roll_up_episodes(events_path=EVENTS_PATH, destination_dir=episode_dir, overwrite=True, show_progress=True)\n",
    "    print(\"Episode roll-up:\", summary)\n",
    "else:\n",
    "    print(\"Episodes already present:\", episode_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92626e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build or load embeddings\n",
    "if REBUILD_EMBEDDINGS or not EMBED_PATH.exists():\n",
    "    model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "    event_ids = []\n",
    "    vectors = []\n",
    "    for start in tqdm(range(0, len(ds), BATCH_SIZE), desc=\"Embedding events\"):\n",
    "        batch = ds[start:start + BATCH_SIZE]\n",
    "        texts = [f\"{EMBED_PASSAGE_PREFIX}{q} {r}\".strip() for q, r in zip(batch[\"question\"], batch[\"response\"]) ]\n",
    "        emb = model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "        vectors.append(emb)\n",
    "        event_ids.extend(batch[\"event_id\"])\n",
    "    vectors = np.vstack(vectors)\n",
    "    np.savez(EMBED_PATH, event_ids=np.array(event_ids), vectors=vectors)\n",
    "    print(\"Wrote\", EMBED_PATH)\n",
    "else:\n",
    "    model = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "    print(\"Using cached embeddings:\", EMBED_PATH)\n",
    "\n",
    "\n",
    "storage.load_event_embeddings_npz(EMBED_PATH)\n",
    "storage.register_event_embedder(lambda text: model.encode([f\"{EMBED_QUERY_PREFIX}{text}\"], convert_to_numpy=True)[0])\n",
    "print('Event embeddings loaded:', len(storage.EVENT_EMBEDDINGS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de985751",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c4eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = os.getenv('ACE_MODEL_NAME', 'Qwen/Qwen2.5-3B-Instruct')\n",
    "DEVICE = os.getenv('ACE_DEVICE', 'auto')\n",
    "CHAT_ID = os.getenv('ACE_CHAT_ID', 'default')\n",
    "TOP_K = int(os.getenv('ACE_TOP_K', '5'))\n",
    "MAX_NEW_TOKENS = int(os.getenv('ACE_MAX_NEW_TOKENS', '400'))\n",
    "EXTRACT_MAX_NEW_TOKENS = int(os.getenv('ACE_EXTRACT_MAX_NEW_TOKENS', '200'))\n",
    "EVENT_CONTEXT_BUDGET_TOKENS = int(os.getenv('ACE_EVENT_CONTEXT_BUDGET_TOKENS', '1200'))\n",
    "TEMPERATURE = float(os.getenv('ACE_TEMPERATURE', '0.2'))\n",
    "TOP_P = float(os.getenv('ACE_TOP_P', '0.9'))\n",
    "REFRESH_EVERY = int(os.getenv('ACE_REFRESH_EVERY', '3'))\n",
    "EPISODE_LOOKBACK_MONTHS = int(os.getenv('ACE_EPISODE_LOOKBACK_MONTHS', '12'))\n",
    "EPISODE_EVENTS_PER_CONTEXT = int(os.getenv('ACE_EPISODE_EVENTS_PER_CONTEXT', '5'))\n",
    "EPISODE_TOP_K = int(os.getenv('ACE_EPISODE_TOP_K', '5'))\n",
    "EPISODE_PREFILTER = os.getenv('ACE_EPISODE_PREFILTER', '1').lower() in {'1','true','yes'}\n",
    "MEMORY_SCORE_THRESHOLD = float(os.getenv('ACE_MEMORY_SCORE_THRESHOLD', '0.35'))\n",
    "ALLOW_GENERAL_FALLBACK = os.getenv('ACE_ALLOW_GENERAL_FALLBACK', '1').lower() in {'1','true','yes'}\n",
    "MODEL_DTYPE = os.getenv('ACE_MODEL_DTYPE', 'float16')\n",
    "LOW_CPU_MEM = os.getenv('ACE_LOW_CPU_MEM', '1').lower() in {'1','true','yes'}\n",
    "FORCE_DOWNLOAD = os.getenv('ACE_FORCE_DOWNLOAD', '0').lower() in {'1', 'true', 'yes'}\n",
    "HF_CACHE_DIR = os.getenv('ACE_HF_CACHE_DIR')\n",
    "STREAM = int(os.getenv('ACE_STREAM', '1'))  # 1=stream tokens, 0=full response\n",
    "\n",
    "MEMORY_SYSTEM_PROMPT = (\n",
    "    'You are a personal assistant with access to prior interactions. '\n",
    "    'Answer only using the provided context; if the answer is not in memory, say so.'\n",
    ")\n",
    "GENERAL_SYSTEM_PROMPT = (\n",
    "    'You are a helpful assistant. Use general knowledge when needed. '\n",
    "    'If memory context is provided, prefer it and cite it.'\n",
    ")\n",
    "def pick_device(choice: str) -> str:\n",
    "    if choice != 'auto':\n",
    "        return choice\n",
    "    if torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    return 'cpu'\n",
    "\n",
    "def should_use_memory(query: str, window, hits, threshold: float) -> bool:\n",
    "    lowered = query.lower()\n",
    "    memory_hints = [\n",
    "        'remember', 'recall', 'what did i', 'what did we', 'what did you',\n",
    "        'earlier', 'previous', 'last time', 'yesterday', 'today', 'last week',\n",
    "        'last month', 'last year', 'ago'\n",
    "    ]\n",
    "    if window is not None:\n",
    "        return True\n",
    "    if any(hint in lowered for hint in memory_hints):\n",
    "        return True\n",
    "    if not hits:\n",
    "        return False\n",
    "    top_score = hits[0].get('score', 0.0)\n",
    "    return top_score >= threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_model_source(model_name, cache_dir, force_download):\n",
    "    if Path(model_name).exists():\n",
    "        print(f'Using local model path: {model_name}')\n",
    "        return model_name, True\n",
    "    if snapshot_download is None:\n",
    "        print('huggingface_hub not available; loading via transformers.')\n",
    "        return model_name, False\n",
    "    if force_download:\n",
    "        print('Force download enabled; downloading model files with per-file progress.')\n",
    "        cache_path = snapshot_download(\n",
    "            model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            local_files_only=False,\n",
    "            resume_download=True,\n",
    "        )\n",
    "        return cache_path, True\n",
    "    try:\n",
    "        cache_path = snapshot_download(\n",
    "            model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            local_files_only=True,\n",
    "        )\n",
    "        print(f'Cache hit: model snapshot already present at {cache_path}')\n",
    "        return cache_path, True\n",
    "    except LocalEntryNotFoundError:\n",
    "        print('Cache miss: downloading model files with per-file progress.')\n",
    "        cache_path = snapshot_download(\n",
    "            model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            local_files_only=False,\n",
    "            resume_download=True,\n",
    "        )\n",
    "        return cache_path, True\n",
    "    except Exception:\n",
    "        print('Cache lookup failed; downloading via transformers.')\n",
    "        return model_name, False\n",
    "\n",
    "device = pick_device(DEVICE)\n",
    "dtype = None\n",
    "if MODEL_DTYPE == 'float16':\n",
    "    dtype = torch.float16\n",
    "elif MODEL_DTYPE == 'bfloat16':\n",
    "    dtype = torch.bfloat16\n",
    "print(f'Loading model {MODEL_NAME} on {device}...')\n",
    "if HF_CACHE_DIR:\n",
    "    print(f'Using Hugging Face cache dir: {HF_CACHE_DIR}')\n",
    "model_source, local_only = resolve_model_source(MODEL_NAME, HF_CACHE_DIR, FORCE_DOWNLOAD)\n",
    "load_kwargs = {\n",
    "    'cache_dir': HF_CACHE_DIR,\n",
    "    'force_download': FORCE_DOWNLOAD,\n",
    "    'local_files_only': local_only,\n",
    "}\n",
    "if dtype is not None:\n",
    "    load_kwargs['dtype'] = dtype\n",
    "if LOW_CPU_MEM:\n",
    "    load_kwargs['low_cpu_mem_usage'] = True\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_source, **load_kwargs)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "print('Loading model weights...')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_source, **load_kwargs)\n",
    "if device != 'cpu':\n",
    "    print(f'Moving model to {device}...')\n",
    "    model.to(device)\n",
    "\n",
    "model.eval()\n",
    "if hasattr(model, 'generation_config') and model.generation_config is not None:\n",
    "    model.generation_config.top_k = 0\n",
    "print('Model ready')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8071ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage.ensure_retrieval_index(refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ccdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional demo: build episodes from faux_events.jsonl\n",
    "# This is for reviewer sanity-checks only and is not used in paper results.\n",
    "\n",
    "# Copy faux_events.jsonl from data/Demo_Dataset/ to data/events/ to run this cell,\n",
    "# or change faux_path below to point to its current location.\n",
    "\n",
    "# NOTE: Remove the faux dataset events and episodes or skip this cell during actual metric evaluations\n",
    "# to avoid contaminating the dataset with synthetic events.\n",
    "\n",
    "\n",
    "\n",
    "# faux_path = REPO_ROOT / 'data' / 'events' / 'faux_events.jsonl'\n",
    "# if faux_path.exists():\n",
    "#     print(f'Building episodes from {faux_path}...')\n",
    "#     print(f'Episodes destination: {storage.STORAGE.episodes}')\n",
    "#     storage.build_episodes_from_jsonl(faux_path, overwrite=False)\n",
    "#     storage.ensure_retrieval_index(refresh=True)\n",
    "#     print('Done.')\n",
    "# else:\n",
    "#     print('faux_events.jsonl not found; skipping demo cell.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be421d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_chat_loop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    storage=storage,\n",
    "    chat_id=CHAT_ID,\n",
    "    top_k=TOP_K,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    extract_max_new_tokens=EXTRACT_MAX_NEW_TOKENS,\n",
    "    event_context_budget_tokens=EVENT_CONTEXT_BUDGET_TOKENS,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    "    refresh_every=REFRESH_EVERY,\n",
    "    episode_lookback_months=EPISODE_LOOKBACK_MONTHS,\n",
    "    episode_events_per_context=EPISODE_EVENTS_PER_CONTEXT,\n",
    "    episode_top_k=EPISODE_TOP_K,\n",
    "    episode_prefilter=EPISODE_PREFILTER,\n",
    "    memory_score_threshold=MEMORY_SCORE_THRESHOLD,\n",
    "    allow_general_fallback=ALLOW_GENERAL_FALLBACK,\n",
    "    memory_system_prompt=MEMORY_SYSTEM_PROMPT,\n",
    "    general_system_prompt=GENERAL_SYSTEM_PROMPT,\n",
    "    stream=STREAM,\n",
    "    should_use_memory_fn=should_use_memory,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83425f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ACE_FINAL",
   "language": "python",
   "name": "ace_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
